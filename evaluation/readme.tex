\hypertarget{diospyros-evaluation}{%
\section{Diospyros Evaluation}\label{diospyros-evaluation}}

This directory contains the evaluation scripts for our ASPLOS 2021
paper, ``Vectorization for Digital Signal Processors via Equality
Saturation''.

This artifact is intended to reproduce the 4 main experimental results
from the paper: - \textbf{Compiling benchmarks (Table 1; Figure 4)}
Compilation and simulated cycle-level performance of 21 kernels (across
4 distinct functions). We compare kernels compiled by Diospyros with
kernels compiled with the vendor's optimizing compiler and optimized
library functions. - \textbf{Translation validation (Section 3)}
Translation validation for all kernels that the scalar specification and
vectorized result (both in our imperative vector domain specific
language) are equivalent. - \textbf{Timeout ablation study (Figure 5)}
Ablation study on a single kernel (10×10 by 10×10 \texttt{MatMul}) over
a range of equality saturation timeouts. - \textbf{Application case
study (Section 5.6)} Speedup of an open source computer vision
application (the \href{https://github.com/sweeneychris/TheiaSfM}{Theia}
structure-from-motion library) with a single kernel compiled by
Diospyros (QR decomposition).

We have split this artifact into two components: 1. \textbf{Diospyros
compiler.} This is our publicly available compiler that produces C/C++
code with intrinsics. This component can be run on the provided
\href{https://www.virtualbox.org/}{VirtualBox} virtual machine, or
installed from source and run locally on the reviewer's machine. 2.
\textbf{Evaluation on licensed instruction set simulator (ISS).} Our
compiler targets the Tensilica Fusion G3, which does not have an
publicly accessible compiler or ISS (the vendor provides free academic
licenses, but the process is not automated). To reproduce the
cycle-level simulation statistics from our paper, we have provided
reviews limited access to our research server (with permission from the
AEC chairs).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{prerequisites}{%
\subsection{Prerequisites}\label{prerequisites}}

\hypertarget{option-1-virtualbox}{%
\subsubsection{Option 1: VirtualBox}\label{option-1-virtualbox}}

If you use the provided VirtualBox virtual machine, it has all
dependencies pre-installed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Download \href{https://www.virtualbox.org/}{VirtualBox} and follow the
  instructions, then login with:
\end{enumerate}

\begin{verbatim}
password: asplos
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Right-click the Deskop and select ``Open in Terminal''.
\item
  Go to the project directory:
\end{enumerate}

\begin{verbatim}
cd diospyros
\end{verbatim}

\hypertarget{option-2-running-locally}{%
\subsubsection{Option 2: Running
locally}\label{option-2-running-locally}}

To run locally, clone this repository and follow the instructions for
installing prerequisites from the top-level README.

\begin{verbatim}
cd diospyos
\end{verbatim}

\hypertarget{using-the-compiler-in-the-virtualbox-or-locally}{%
\subsection{Using the compiler in the VirtualBox or
locally}\label{using-the-compiler-in-the-virtualbox-or-locally}}

First, run make to ensure the compiler is up-to-date:

\begin{verbatim}
make
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{generating-cc-with-intrinsics-on-the-vm-or-locally}{%
\subsubsection{Generating C/C++ with Intrinsics (on the VM or
locally)}\label{generating-cc-with-intrinsics-on-the-vm-or-locally}}

To start, we will generate most of the compiled C/C++ with intrinsics
independently from the research server (either on the provided VM or
locally). To skip running on the licensed simulator, we pass the
\texttt{-\/-skiprun} flag to the following commands.

First, to sanity check the setup, run the following test command, which
compiles the smallest size of each unique kernel with a 10 second
timeout for each:

\hypertarget{time-estimate-30-seconds}{%
\paragraph{Time estimate: 30 seconds}\label{time-estimate-30-seconds}}

\begin{verbatim}
python3 evaluation/eval_benchmarks.py --timeout 10 --skiprun --test -o test-results
\end{verbatim}

This produces \texttt{*.c} files with vector intrinsics, along with
metadata used for downstream translation validation, in a new
\texttt{test-results} directory. The directory is structured with
subdirectories for each function and size:

\begin{verbatim}
- test-results
    - 2d-conv
        - <sizes>
    - mat-mul
        - <sizes>
    - q-prod
        - <sizes>
    - qr-decomp
        - <sizes>
\end{verbatim}

Within each size, there are the following files:

\begin{verbatim}
- egg-kernel.c.            : vectorized C code with intrinsics.
- params.json              : input size and vectorization parameters.
- spec.rkt                 : specification lifted with symbolic evaluation, in DSL.
- res.rkt                  : vectorized result of equality saturation, in DSL.
- outputs.rkt, prelude.rkt : metadata for downstreaam translation validation.
- stats.json               : summary statistics from compilation, including wall clock time and memory usage.
\end{verbatim}

Once that succeeds, we can run the benchmarks with the default 180
second timeout. For now, we suggest skipping the one kernel
(\texttt{4x4\ QRDecomp}) that requires 38 GB of memory (as documented in
Table 1), since it is infeasible to run in a VM. If you are running
locally on a machine with sufficient memory, you can include this
benchmark by omitting the \texttt{-\/-skiplargemem} flag.

\hypertarget{time-estimate-45-minutes-5-hours-if-no---skiplargemem}{%
\paragraph{\texorpdfstring{Time estimate: 45 minutes (+5 hours if no
\texttt{-\/-skiplargemem})}{Time estimate: 45 minutes (+5 hours if no -\/-skiplargemem)}}\label{time-estimate-45-minutes-5-hours-if-no---skiplargemem}}

\begin{verbatim}
python3 evaluation/eval_benchmarks.py --skiprun --skiplargemem -o results
\end{verbatim}

The results here follow the same pattern of files as specified above,
but for 20/21 total kernel sizes.

\hypertarget{translation-validation}{%
\subsubsection{Translation validation}\label{translation-validation}}

To run translation validation on the results we just generated, we will
rerun the same script and pass \texttt{-\/-validation} and the
\texttt{-\/-skipsynth} flag to tell the script to not regenerate the
results.

\hypertarget{time-estimate-10-minutes}{%
\paragraph{Time estimate: 10 minutes}\label{time-estimate-10-minutes}}

\begin{verbatim}
python3 evaluation/eval_benchmarks.py --skiprun --skipsynth --validation -o results
\end{verbatim}

The line
\texttt{Translation\ validation\ successful!\ \textless{}N\textgreater{}\ elements\ equal}
will be printed to the Terminal for each kernel that passes.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{running-code-on-the-licensed-instruction-set-simulator-on-the-research-server}{%
\subsection{Running code on the licensed Instruction Set Simulator (on
the research
server)}\label{running-code-on-the-licensed-instruction-set-simulator-on-the-research-server}}

\hypertarget{setting-up-your-directory}{%
\subsubsection{Setting up your
directory}\label{setting-up-your-directory}}

First, SSH into our research server using the provided credentials:

\hypertarget{on-the-vmlocally}{%
\paragraph{On the VM/locally}\label{on-the-vmlocally}}

\begin{verbatim}
ssh <user>@<server address>
\end{verbatim}

On the research server, cd to the project directory:

\hypertarget{on-the-server}{%
\paragraph{On the server}\label{on-the-server}}

\begin{verbatim}
cd /data/asplos-aec
\end{verbatim}

Here, we want to create a new folder per reviewer to avoid overwriting
each other's work. Run the following, replacing
\texttt{\textless{}letter\textgreater{}} with the reviewer letter you
are assigned in HotCRP (for example, \texttt{mkdir\ reviewer-A}). Then,
copy the \texttt{diospyros} repository into your new directory and
\texttt{cd} there:

\hypertarget{on-the-server-1}{%
\paragraph{On the server}\label{on-the-server-1}}

\begin{verbatim}
mkdir reviewer-<letter>
cp -r diospyros reviewer-<letter>/diospyros
cd reviewer-<letter>/diospyros
\end{verbatim}

Now, back on your original machine (the VM or locally), run the
following command to copy the data you just generated to your new
directory on the research server (replacing
\texttt{\textless{}letter\textgreater{}} with your letter):

\hypertarget{on-the-vmlocally-in-the-diospryros-root-directory-time-estimate-1-minute}{%
\paragraph{\texorpdfstring{On the VM/locally, in the \texttt{diospryros}
root directory (Time estimate: 1
minute)}{On the VM/locally, in the diospryros root directory (Time estimate: 1 minute)}}\label{on-the-vmlocally-in-the-diospryros-root-directory-time-estimate-1-minute}}

\begin{verbatim}
scp -r results <user>@<server address>:/data/asplos-aec/reviewer-<letter>/diospyros/results
\end{verbatim}

Back on the server, check that you have your results:

\hypertarget{on-the-server-2}{%
\paragraph{On the server}\label{on-the-server-2}}

\begin{verbatim}
ls results/*
\end{verbatim}

You should see the 4 kernel directories and respective sizes:

\begin{verbatim}
2d-conv     mat-mul     q-prod      qr-decomp
\end{verbatim}

\hypertarget{running-the-instruction-set-simulator}{%
\subsubsection{Running the Instruction Set
Simulator}\label{running-the-instruction-set-simulator}}

Now, we can actually run the generated kernels on the vendor's
instruction set simulator. We pass the \texttt{-\/-skipsynth} flag to
avoid re-running synthesis and compilation to C with intrinsics.

\hypertarget{time-estimate-minutes}{%
\paragraph{Time estimate: ? minutes}\label{time-estimate-minutes}}

\begin{verbatim}
python3 evaluation/eval_benchmarks.py --skipsynth -d results
\end{verbatim}

This will add a file \texttt{egg-kernel.csv} to each subdirectory of
\texttt{results} with cycle-level performance for each kernel and
corresponding baseline.

\hypertarget{seeing-the-results}{%
\subsubsection{Seeing the Results}\label{seeing-the-results}}

Now that we have collected the data, the next step is to analyze the
results to draw the charts and tables you see in the ASPLOS paper.
First, use \texttt{chart\_benchmarks.py} to generate the plots:

\begin{verbatim}
python3 evaluation/chart_benchmarks.py -d results
\end{verbatim}

This produces the following files:

\begin{verbatim}
all_benchmarks.csv          : Combined all individual benchmark runs into one CSV (Feeds Table 1)
all_bechmarks_chart.pdf     : Charting graph for all benchmarks (Figure 4)
extended_abstract_chart.pdf : Charting small graph for extended abstract
\end{verbatim}

The produced \texttt{all\_benchmarks.csv} file contains all the raw data
that went into the plots. You can look at it directly if you're curious
about specific numbers. To see some statistics about the compilation
process, run the \texttt{benchtbl.py} script:

\begin{verbatim}
python3 evaluation/benchtbl.py --plain
\end{verbatim}

This script reads \texttt{all\_benchmarks.csv} to make a table like
Table 1 in the ASPLOS paper. The \texttt{-\/-plain} flag emits a
plain-text table for reading directly; omit this flag to generate a
LaTeX fragment instead.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{theia-case-study}{%
\subsubsection{Theia Case Study}\label{theia-case-study}}

The ASPLOS paper puts the QR decomposition kernel into context by
measuring the impact on performance in an application: the
\href{https://github.com/sweeneychris/TheiaSfM}{Theia}
structure-from-motion library. The \texttt{theia} subdirectory in this
evaluation package contains the code necessary to reproduce those
end-to-end results.

The first step is to get the generated C code for the QR decomposition
kernel. Copy the \texttt{egg\_kernel.c} file from our previously
generated code for an \texttt{N=3} \texttt{vecwidth=4} \texttt{QRDecomp}
to the Theia directory:

\begin{verbatim}
cp results/qr-decomp/3_4r/egg-kernel.c theia/egg-kernel.c
\end{verbatim}

\textbf{Time estimate: 1 minute.}

Then, type \texttt{make\ run\ -C\ evaluation/theia} to compile and
execute both versions of the \texttt{DecomposeProjectionMatrix}
function: one using Eigen (as the original open-source code does) and
one using the Diospyros-generated QR kernel. You can visually check the
outputs to make sure they match.

\textbf{Time estimate: 10 seconds.}

To post-process this output into the final numbers you see in the paper,
pipe it into the \texttt{dpmresults.py} analysis script:

\begin{verbatim}
make run -C evaluation/theia | python3 evaluation/dpmresults.py
\end{verbatim}

This will produce a JSON document with three values: the cycle count for
the Eigen- and Diospyros-based executions, and the speedup (which is
just the ratio of the two cycle counts). You can see the cycle counts
and speedup number in the ASPLOS paper at the end of Section 5.6.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{timeout-ablation-study}{%
\subsubsection{Timeout Ablation Study}\label{timeout-ablation-study}}

Our ASPLOS paper studies the effect of changing the timeout of the
equality saturation solver on the quality of the generated solution. The
ablation study varies the timeout given to our solver and reports the
number of cycles taken by the design. We assume that this experiment is
performed on our research server with the Xtensa compiler and simulator
available.

The process of reproducing has two steps: 1. Run the study with
different timeouts and generate executable code. 2. Simulate the
executable code with the simulator.

The script \texttt{evaluation/ablation/ablation-exp-gen.py} can be used
to generate executable vectorized solutions:

\begin{verbatim}
python3 evaluation/ablation/ablation-exp-gen.py -p evaluation/ablation/params/mat-mul-large -o exp-out -t 10 30 60 120 180
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{-p}: Specifies the parameter file, which itself describes the
  size of matrices used in a matrix-multiply kernel.
\item
  \texttt{-o}: Location of the output folder
\item
  \texttt{-t}: A list of the timeouts to run the solver with.
\end{itemize}

Once the script is finished running, run the following to collect the
data:

\begin{verbatim}
cd exp-out
A_ROWS=10 A_COLS=10 B_ROWS=10 B_COLS=10 ./run_all.sh
cd -
\end{verbatim}

This will generate \texttt{exp-out/ablation.csv}

Finally, run the following to generate the ablation study chart:

\begin{verbatim}
python3 evaluation/ablation/ablation_chart.py exp-out/ablation.csv
\end{verbatim}

This will generate \texttt{ablation.pdf}
